{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attempting to solve the Multi-Armed Bandit problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.contrib.slim as slim\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>The Bandits</h3>\n",
    "Here we define our bandits. For this example we are using a four-armed bandit. The pullBandit function generates a random number from a normal distribution with a mean of 0. The lower the bandit number, the more likely a positive reward will be returned. We want our agent to learn to always choose the bandit that will give that positive reward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "b1 = round(5 * np.random.randn(), 2)\n",
    "b2 = round(5 * np.random.randn(), 2)\n",
    "b3 = round(5 * np.random.randn(), 2)\n",
    "b4 = round(5 * np.random.randn(), 2)\n",
    "\n",
    "bandits = [b1, b2, b3, b4]\n",
    "num_bandits = len(bandits)\n",
    "\n",
    "def pullBandit(bandit):\n",
    "    result = np.random.randn(1)\n",
    "    if result > bandit:\n",
    "        return 1\n",
    "    else:\n",
    "        return -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>The Agent</h3>\n",
    "The code below established our simple neural agent. It consists of a set of values for each of the bandits. Each value is an estimate of the value of the return from choosing the bandit. We use a policy gradient method to update the agent by moving the value for the selected action toward the recieved reward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "weights = tf.Variable(tf.ones([num_bandits]))\n",
    "chosen_action = tf.argmax(weights, 0)\n",
    "\n",
    "reward_holder = tf.placeholder(shape=[1], dtype=tf.float32)\n",
    "action_holder = tf.placeholder(shape=[1], dtype=tf.int32)\n",
    "responsible_weight = tf.slice(weights, action_holder, [1])\n",
    "loss = -(tf.log(responsible_weight) * reward_holder)\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.001)\n",
    "update = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Training the Agent</h3>\n",
    "We will train our agent by taking actions in our environment, and recieving rewards. Using the rewards and actions, we can know how to properly update our network in order to more often choose actions that will yield the highest rewards over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bandits: [14.03, 4.06, -1.53, -0.43]\n",
      "Running reward for the 4 bandits: [0. 0. 1. 0.]\n",
      "Running reward for the 4 bandits: [ 0.  0. 42. -1.]\n",
      "Running reward for the 4 bandits: [-1.  0. 87. -3.]\n",
      "Running reward for the 4 bandits: [ -1.   0. 124.  -4.]\n",
      "Running reward for the 4 bandits: [ -1.  -1. 167.  -2.]\n",
      "Running reward for the 4 bandits: [ -3.  -1. 203.  -2.]\n",
      "Running reward for the 4 bandits: [ -5.  -3. 235.  -4.]\n",
      "Running reward for the 4 bandits: [ -6.  -3. 274.  -2.]\n",
      "Running reward for the 4 bandits: [ -7.  -4. 311.   1.]\n",
      "Running reward for the 4 bandits: [ -9.  -7. 349.   2.]\n",
      "Running reward for the 4 bandits: [-12.  -8. 386.   3.]\n",
      "Running reward for the 4 bandits: [-12.  -9. 422.   4.]\n",
      "Running reward for the 4 bandits: [-13. -11. 460.   5.]\n",
      "Running reward for the 4 bandits: [-13. -13. 498.   7.]\n",
      "Running reward for the 4 bandits: [-14. -15. 534.   8.]\n",
      "Running reward for the 4 bandits: [-15. -15. 571.   8.]\n",
      "Running reward for the 4 bandits: [-15. -16. 604.   8.]\n",
      "Running reward for the 4 bandits: [-15. -20. 648.   8.]\n",
      "Running reward for the 4 bandits: [-16. -20. 691.   8.]\n",
      "Running reward for the 4 bandits: [-18. -20. 734.   9.]\n",
      "The agent thinks bandit 3 is the most promising....\n",
      "...and it was right!\n"
     ]
    }
   ],
   "source": [
    "print(\"Bandits:\", bandits)\n",
    "\n",
    "total_episodes = 1000\n",
    "total_reward = np.zeros(num_bandits) #'scoreboard' for bandits\n",
    "e = 0.1 #chance of taking random action\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "\n",
    "    sess.run(init)\n",
    "\n",
    "    for i in range(total_episodes):\n",
    "        \n",
    "        if np.random.rand(1) < e: #for random action\n",
    "            action = np.random.randint(num_bandits)\n",
    "        else:\n",
    "            action = sess.run(chosen_action)\n",
    "        \n",
    "        # Get reward from chosen Bandit\n",
    "        reward = pullBandit(bandits[action])\n",
    "        \n",
    "        _, resp, ww = sess.run([update, responsible_weight, weights],\n",
    "                              feed_dict={reward_holder:[reward], action_holder:[action]})\n",
    "        \n",
    "        total_reward[action] += reward\n",
    "        \n",
    "        if i % 50 == 0:\n",
    "            print (\"Running reward for the \" + str(num_bandits) + \" bandits: \" + str(total_reward))\n",
    "        \n",
    "\n",
    "print (\"The agent thinks bandit \" + str(np.argmax(ww)+1) + \" is the most promising....\")\n",
    "if np.argmax(ww) == np.argmax(-np.array(bandits)):\n",
    "    print (\"...and it was right!\")\n",
    "else:\n",
    "    print (\"...and it was wrong!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>The Contextual Bandits</h3>\n",
    "Here we define our contextual bandits. In this example, we are using three four-armed bandit. What this means is that each bandit has four arms that can be pulled. Each bandit has different success probabilities for each arm, and as such requires different actions to obtain the best result. The pullBandit function generates a random number from a normal distribution with a mean of 0. The lower the bandit number, the more likely a positive reward will be returned. We want our agent to learn to always choose the bandit-arm that will most often give a positive reward, depending on the Bandit presented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class contextual_bandit():\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.state = 0\n",
    "        self.bandits = np.array([[0.2,0,-0.0,-5],[0.1,-5,1,0.25],\n",
    "                                 [-5,5,5,5]])\n",
    "        self.num_bandits = self.bandits.shape[0]\n",
    "        self.num_actions = self.bandits.shape[1]\n",
    "        \n",
    "    def getBandit(self):\n",
    "        self.state = np.random.randint(0, len(self.bandits)) #Returns a random state for each episode\n",
    "        return self.state\n",
    "    \n",
    "    def pullArm(self, action):\n",
    "        bandit = self.bandits[self.state, action]\n",
    "        result = np.random.randn(1)\n",
    "        if result > bandit:\n",
    "            return 1\n",
    "        else:\n",
    "            return -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>The Policy-Based Agent</h3>\n",
    "The code below established our simple neural agent. It takes as input the current state, and returns an action. This allows the agent to take actions which are conditioned on the state of the environment, a critical step toward being able to solve full RL problems. The agent uses a single set of weights, within which each value is an estimate of the value of the return from choosing a particular arm given a bandit. We use a policy gradient method to update the agent by moving the value for the selected action toward the recieved reward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class agent():\n",
    "    \n",
    "    def __init__(self, lr, s_size, a_size):\n",
    "        self.state_in = tf.placeholder(shape=[1], dtype=tf.int32)\n",
    "        state_in_OH = slim.one_hot_encoding(self.state_in, s_size)\n",
    "        output = slim.fully_connected(state_in_OH, a_size, biases_initializer=None,\n",
    "                                      activation_fn=tf.nn.sigmoid, \n",
    "                                      weights_initializer=tf.ones_initializer())\n",
    "        self.output = tf.reshape(output, [-1])\n",
    "        self.chosen_action = tf.argmax(self.output, 0)\n",
    "        \n",
    "        # training procedure\n",
    "        self.reward_holder = tf.placeholder(shape=[1], dtype=tf.float32)\n",
    "        self.action_holder = tf.placeholder(shape=[1], dtype=tf.int32)\n",
    "        self.responsible_weight = tf.slice(self.output, self.action_holder, [1])\n",
    "        self.loss = -(tf.log(self.responsible_weight) * self.reward_holder)\n",
    "        optimizer = tf.train.GradientDescentOptimizer(learning_rate=lr)\n",
    "        self.update = optimizer.minimize(self.loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Training the Agent</h3>\n",
    "We will train our agent by getting a state from the environment, take an action, and recieve a reward. Using these three things, we can know how to properly update our network in order to more often choose actions given states that will yield the highest rewards over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean reward for each of the 3 bandits: [0.   0.   0.25]\n",
      "Mean reward for each of the 3 bandits: [36.   33.5  36.25]\n",
      "Mean reward for each of the 3 bandits: [66.5  71.5  71.75]\n",
      "Mean reward for each of the 3 bandits: [111.   109.5  103.75]\n",
      "Mean reward for each of the 3 bandits: [151.5  147.   135.75]\n",
      "Mean reward for each of the 3 bandits: [193.75 182.25 171.25]\n",
      "Mean reward for each of the 3 bandits: [233.   221.   205.25]\n",
      "Mean reward for each of the 3 bandits: [274.75 256.5  240.  ]\n",
      "Mean reward for each of the 3 bandits: [312.5  297.   271.25]\n",
      "Mean reward for each of the 3 bandits: [348.75 334.25 306.25]\n",
      "Mean reward for each of the 3 bandits: [389.   372.75 336.5 ]\n",
      "Mean reward for each of the 3 bandits: [427.5  410.75 369.5 ]\n",
      "Mean reward for each of the 3 bandits: [467.5  450.75 402.5 ]\n",
      "Mean reward for each of the 3 bandits: [505.5  487.   436.25]\n",
      "Mean reward for each of the 3 bandits: [539.75 529.   472.5 ]\n",
      "Mean reward for each of the 3 bandits: [582.25 568.25 503.75]\n",
      "Mean reward for each of the 3 bandits: [623.25 610.   536.5 ]\n",
      "Mean reward for each of the 3 bandits: [658.5  656.75 572.  ]\n",
      "Mean reward for each of the 3 bandits: [697.75 692.25 606.25]\n",
      "Mean reward for each of the 3 bandits: [736.25 730.5  644.5 ]\n",
      "The agent thinks action 4 for bandit 1 is the most promising....\n",
      "...and it was right!\n",
      "The agent thinks action 2 for bandit 2 is the most promising....\n",
      "...and it was right!\n",
      "The agent thinks action 1 for bandit 3 is the most promising....\n",
      "...and it was right!\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "cBandit = contextual_bandit()\n",
    "myAgent = agent(lr=0.001, s_size=cBandit.num_bandits, a_size=cBandit.num_actions)\n",
    "weights = tf.trainable_variables()[0]\n",
    "\n",
    "total_episodes = 10000\n",
    "total_reward = np.zeros([cBandit.num_bandits, cBandit.num_actions])\n",
    "error = 0.1 # chance of taking a random action\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    sess.run(init)\n",
    "    \n",
    "    for i in range(total_episodes):\n",
    "        s = cBandit.getBandit()\n",
    "        if np.random.rand(1) < error:\n",
    "            action = np.random.randint(cBandit.num_actions)\n",
    "        else:\n",
    "            action = sess.run(myAgent.chosen_action, feed_dict={myAgent.state_in:[s]})\n",
    "        \n",
    "        reward = cBandit.pullArm(action)\n",
    "        \n",
    "        feed_dict = {myAgent.reward_holder:[reward], myAgent.action_holder:[action], myAgent.state_in:[s]}\n",
    "        _, ww = sess.run([myAgent.update, weights], feed_dict=feed_dict)\n",
    "        \n",
    "        total_reward[s, action] += reward\n",
    "        if i % 500 == 0:\n",
    "            print (\"Mean reward for each of the \" + str(cBandit.num_bandits) + \" bandits: \" + str(np.mean(total_reward,axis=1)))\n",
    "            \n",
    "# return results\n",
    "for a in range (cBandit.num_bandits):\n",
    "    print (\"The agent thinks action \" + str(np.argmax(ww[a])+1) + \" for bandit \" + str(a+1) + \" is the most promising....\")\n",
    "    if np.argmax(ww[a]) == np.argmin(cBandit.bandits[a]):\n",
    "        print (\"...and it was right!\")\n",
    "    else:\n",
    "        print (\"...and it was wrong!\")               "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
